#
# Annotate TSV ranks and convert to Json / Jsonl DPR encode format.
#

import json
import random
import argparse
from tqdm import tqdm
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from functools import partial
from multiprocessing import Pool

import datasets

from bowdpr.utils.data_utils import read_corpus, build_corpus_idx_to_row

from qa_utils import SimpleTokenizer, has_answers

random.seed(42)

def wc_count(file_name):
    import subprocess
    out = subprocess.getoutput("wc -l %s" % file_name)
    return int(out.split()[0])

def process_tsv_file(tsv_ranks_path: str, depth: int=1000):
    q_p_dict = {}
    print(f"Reading idx from {tsv_ranks_path}")
    for _, line in enumerate(tqdm(open(tsv_ranks_path, 'r'), total=wc_count(tsv_ranks_path))):
        line: list = line.split('\t')
        if len(line) == 4:
            line.pop(1)
        qid: str = line[0].strip()
        pid: str = line[1].strip()
        score: str = float(line[2])     # This score is generated by dual-encoder
        if qid not in q_p_dict:
            q_p_dict[qid] = [(pid, score)]
        else:
            q_p_dict[qid].append((pid, score))
    for k, v in q_p_dict.items():
        q_p_dict[k] = sorted(v, key=lambda x: x[1], reverse=True)
        q_p_dict[k] = q_p_dict[k][:depth]
    return q_p_dict


def annotate(
    item: dict, 
    query_dataset: datasets.Dataset,
    idx_to_query: Dict[str, int],
    passage_dataset: datasets.Dataset,
    idx_to_passage: Dict[str, int],
    tokenizer: SimpleTokenizer, 
    regex=False,
    save_text=False,        # Whether to save texts for passages
):
    """Format: 
    [
        {
            "query_id": "...",
            "query": "...",
            "answers": ["...", "...", ... ],
            "contexts": [
                {
                    "docid": "...", # passage id from database tsv file
                    "title": "",
                    "text": "....",
                    "score": "...",  # retriever score
                    "has_answer": true|false
        },
    ]
    """
    query_id, passage_score_list = item["query_id"], item["passage_score_list"]
    query_entry: dict = query_dataset[idx_to_query[query_id]]
    annotated = {
        "query_id": query_id,
        "query": query_entry["text"],
        "answers": query_entry["answers"],
        "contexts": [],
    }
    
    for (passage_id, score) in passage_score_list:
        passage_entry: dict = passage_dataset[idx_to_passage[passage_id]]
        curr_ctx = dict()
        curr_ctx["docid"] = passage_id
        if save_text:
            curr_ctx["title"] = passage_entry.get("title", "")
            curr_ctx["text"] = passage_entry["text"]      
        curr_ctx["score"] = score
        curr_ctx["has_answer"] = has_answers(passage_entry["text"], query_entry["answers"], tokenizer, regex)
        annotated["contexts"].append(curr_ctx)
    
    return annotated


def annotate_and_build_hn(
    item: dict, 
    query_dataset: datasets.Dataset,
    idx_to_query: Dict[str, int],
    passage_dataset: datasets.Dataset,
    idx_to_passage: Dict[str, int],
    qrels_dataset: datasets.Dataset,
    idx_to_qrels: Dict[str, int],
    tokenizer: SimpleTokenizer, 
    regex=False,
    save_text=False,        # Whether to save texts for passages
    depth: int = 100,
    min_neg_size: int = 4,
):
    """Format: 
    [
        {
            "query_id": "...",
            "query": "...",
            "answers": ["...", "...", ... ],
            "positive_passages": [
                {
                    "docid": "...", # passage id from database tsv file
                    "title": "",
                    "text": "....",
                },
                ...
            ],
            "negative_passages": [
                {
                    "docid": "...", # passage id from database tsv file
                    "title": "",
                    "text": "....",
                },
                ...
            ],
        },
    ]
    """
    query_id, passage_score_list = item["query_id"], item["passage_score_list"]
    query_entry: dict = query_dataset[idx_to_query[query_id]]
    annotated = {
        "query_id": query_id,
        "query": query_entry["text"],
        "answers": query_entry["answers"],
        "positive_passages": [],
        "negative_passages": [],
    }

    # BM25 Training sets has duplicated passages
    # Here we deduplicate them
    positive_doc_idxs: List[str] = list()
    for pos_psg in qrels_dataset[idx_to_qrels[query_id]]["positive_passages"]:
        if pos_psg["docid"] not in positive_doc_idxs:
            annotated["positive_passages"].append(pos_psg)
            positive_doc_idxs.append(pos_psg["docid"])
    
    for (passage_id, score) in passage_score_list[:depth]:
        passage_entry: dict = passage_dataset[idx_to_passage[passage_id]]
        curr_ctx = dict()
        curr_ctx["docid"] = passage_id
        if save_text:
            curr_ctx["title"] = passage_entry.get("title", "")
            curr_ctx["text"] = passage_entry["text"]
        has_answer = has_answers(passage_entry["text"], query_entry["answers"], tokenizer, regex)
        if has_answer:
            if passage_id not in positive_doc_idxs:
                annotated["positive_passages"].append(curr_ctx)
                positive_doc_idxs.append(passage_id)
        else:
            annotated["negative_passages"].append(curr_ctx)
      
    random.shuffle(annotated["negative_passages"])

    if len(annotated["positive_passages"]) < 1 or len(annotated["negative_passages"]) < min_neg_size:
        return None
    
    return annotated


def stream_q_p_dict(q_p_dict: Dict[str, List[Tuple[str, float]]]):
    for q, p_score_list in q_p_dict.items():
        yield {
            "query_id": q,
            "passage_score_list": p_score_list
        }


def main(
    tsv_ranks_path: str,
    query_collection: Optional[str],        # String Path to query corpus
    passage_collection: Optional[str],      # String Path to passage corpus
    output_path: str,
    qrels_reference_collection: Optional[str] = None,
    regex: bool = False,
    save_text: bool = False,
    build_hn: bool = False,
    hn_depth: int = 100,
    format: str = "json",
):
    tokenizer = SimpleTokenizer()
    # Load query corpus
    query_dataset: datasets.Dataset = read_corpus(query_collection)
    idx_to_query: Dict[str, int] = build_corpus_idx_to_row(query_dataset)

    # Load passage corpus
    passage_dataset: datasets.Dataset = read_corpus(passage_collection)
    idx_to_passage: Dict[str, int] = build_corpus_idx_to_row(passage_dataset)

    # Read tsv ranks
    q_p_dict: Dict[str, List[Tuple[str, float]]] = process_tsv_file(tsv_ranks_path)

    if build_hn:
        # Load reference training sets
        qrels_dataset: datasets.Dataset = read_corpus(qrels_reference_collection)
        if "negative_passages" in qrels_dataset.column_names:
            qrels_dataset = qrels_dataset.remove_columns("negative_passages")
        idx_to_qrels: Dict[str, int] = build_corpus_idx_to_row(qrels_dataset)
        annotate_func = partial(annotate_and_build_hn,
                                query_dataset=query_dataset,
                                idx_to_query=idx_to_query,
                                passage_dataset=passage_dataset,
                                idx_to_passage=idx_to_passage,
                                qrels_dataset=qrels_dataset,
                                idx_to_qrels=idx_to_qrels,
                                tokenizer=tokenizer,
                                regex=regex,
                                save_text=save_text,
                                depth=hn_depth,
                            )
    else:
        annotate_func = partial(annotate,
                                query_dataset=query_dataset,
                                idx_to_query=idx_to_query,
                                passage_dataset=passage_dataset,
                                idx_to_passage=idx_to_passage,
                                tokenizer=tokenizer,
                                regex=regex,
                                save_text=save_text,
                            )

    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    f = open(output_path, 'w')
    annotates = list()

    # Multiprocess Annotate
    with Pool() as p:
        rets = p.imap(
                annotate_func,
                stream_q_p_dict(q_p_dict),
                chunksize=500,
            )
        for item in tqdm(rets, total=len(q_p_dict)):
            if item is not None:
                if format == "json":
                    annotates.append(item)
                elif format == "jsonl":
                    f.write(json.dumps(item) + '\n')
                else:
                    raise NotImplementedError()

    if format == "json":
        json.dump(annotates, f, indent=4, ensure_ascii=False)

    f.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--tsv_ranks_path', type=str, metavar='path',
                        help="Path to tsv ranks produced by retriever.")
    parser.add_argument('--query_collection', type=str, help="Path to query collections.")
    parser.add_argument('--passage_collection', type=str, help="Path to passage collections.")
    parser.add_argument('--output_path', type=str, help="Where to save the annotated outputs or hard negative training json/jsonl.")
    parser.add_argument('--qrels_reference_collection', type=str, default=None, help="Path to a reference query collections which holds positive passages.")
    parser.add_argument('--regex', action='store_true', help="regex match")
    parser.add_argument('--save_text', action='store_true', help="Whether to save passage texts. Enable this will consume a lot of spaces.")
    parser.add_argument('--build_hn', action='store_true', help="Save format will be hard negative training json/jsonl.")
    parser.add_argument('--hn_depth', type=int, default=100, help="HN depth.")
    parser.add_argument('--format', type=str, default="jsonl", choices=["json", "jsonl"], help="Save format of `output_path`.")
    args = parser.parse_args()

    main(
        tsv_ranks_path=args.tsv_ranks_path,
        query_collection=args.query_collection,
        passage_collection=args.passage_collection,
        output_path=args.output_path,
        qrels_reference_collection=args.qrels_reference_collection,
        regex=args.regex,
        save_text=args.save_text,
        build_hn=args.build_hn,
        hn_depth=args.hn_depth,
        format=args.format,
    )

