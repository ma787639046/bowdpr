import os
from tqdm import tqdm
from typing import List, Tuple, Dict

import datasets
from datasets import load_dataset

def wc_count(file_name):
    import subprocess
    out = subprocess.getoutput("wc -l %s" % file_name)
    return int(out.split()[0])

def build_corpus_idx_to_row(dataset: datasets.Dataset):
    """ Build a dict on memory of corpus id -> row id of hfdataset """
    idx_to_corpus = dict()
    for row_id, corpus_id in enumerate(dataset["_id"]):
        idx_to_corpus[corpus_id] = row_id
    return idx_to_corpus

def read_corpus(corpus_name_or_path: str):
    """ Load HFDataset from local or online """
    corpus_path = None
    # Local file or folders
    if os.path.exists(corpus_name_or_path):
        if os.path.isdir(corpus_name_or_path):
            files = os.listdir(corpus_name_or_path)
            corpus_path = [
                os.path.join(corpus_name_or_path, f)
                for f in files
                if f.endswith('json') or f.endswith('jsonl')
            ]
        else:
            corpus_path = [corpus_name_or_path]
        
        dataset_name = 'json'
        dataset_split = 'train'
        dataset_language = 'default'
    # Online huggingface dataset
    else:
        info = corpus_name_or_path.split('/')
        dataset_split = info[-1] if len(info) == 3 else 'train'
        dataset_name = "/".join(info[:-1]) if len(info) == 3 else '/'.join(info)
        dataset_language = 'default'
        if ':' in dataset_name:
            dataset_name, dataset_language = dataset_name.split(':')
    
    dataset = load_dataset(
        dataset_name,
        dataset_language,
        data_files=corpus_path,
        split=dataset_split
    )

    # Parse tevatron format Jsonl text column names to sentence transformers format
    for _original_column_name, _new_column_name in [("query_id", "_id"), ("docid", "_id"), ("query", "text")]:
        if _original_column_name in dataset.column_names:
            dataset = dataset.rename_column(_original_column_name, _new_column_name)
    
    return dataset

def process_tsv_file(tsv_ranks_path: str, depth: int=1000):
    q_p_dict = {}
    ret = []    # (qid, pid) pairs
    print(f"Reading idx from {tsv_ranks_path}")
    for _, line in enumerate(tqdm(open(tsv_ranks_path, 'r'), total=wc_count(tsv_ranks_path))):
        line: list = line.split('\t')
        if len(line) == 4:
            line.pop(1)
        qid: str = line[0].strip()
        pid: str = line[1].strip()
        score: str = float(line[2])     # This score is generated by dual-encoder
        if qid not in q_p_dict:
            q_p_dict[qid] = [(pid, score)]
        else:
            q_p_dict[qid].append((pid, score))
    for k, v in q_p_dict.items():
        q_p_dict[k] = sorted(v, key=lambda x: x[1], reverse=True)
        q_p_dict[k] = q_p_dict[k][:depth]
        ret.extend([(k, pid) for pid, score in q_p_dict[k]]) 
    return ret