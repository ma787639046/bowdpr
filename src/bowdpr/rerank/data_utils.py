#!/usr/bin/python
# -*- encoding: utf-8 -*-
'''
Training datasets.

@Time    :   2023/11/06
@Author  :   Ma (Ma787639046@outlook.com)
'''
import os
import glob
import random
from tqdm import tqdm
from dataclasses import dataclass
from typing import List, Tuple, Dict
from torch.utils.data import Dataset

import datasets
from datasets import load_dataset
from transformers import PreTrainedTokenizer, BatchEncoding, DataCollatorWithPadding

from .arguments import DataArguments
from ..trainer import ContrastiveTrainer

import logging
logger = logging.getLogger(__name__)

def wc_count(file_name):
    import subprocess
    out = subprocess.getoutput("wc -l %s" % file_name)
    return int(out.split()[0])

class TrainDataset(Dataset):
    """ Wrapper for Sampling Positive / Negative Passages """
    def __init__(
            self,
            data_args: DataArguments,
            dataset: datasets.Dataset,
            trainer: ContrastiveTrainer = None,
    ):
        self.train_data = dataset
        self.trainer = trainer
        self.data_args = data_args

    def __len__(self):
        return len(self.train_data) 

    def __getitem__(self, item) -> Dict[str, any]:
        group = self.train_data[item]
        _hashed_seed = hash(item + self.trainer.args.seed)

        epoch = int(self.trainer.state.epoch)

        qry = group['query']

        group_positives = group['positive_passages']
        group_negatives = group['negative_passages']

        if self.data_args.positive_passage_no_shuffle:
            pos_psg = group_positives[0]
        else:
            pos_psg = group_positives[(_hashed_seed + epoch) % len(group_positives)]

        negative_size = self.data_args.train_n_passages - 1
        if len(group_negatives) < negative_size:
            negs = random.choices(group_negatives, k=negative_size)
        elif self.data_args.train_n_passages == 1:
            negs = []
        elif self.data_args.negative_passage_no_shuffle:
            negs = group_negatives[:negative_size]
        else:
            _offset = epoch * negative_size % len(group_negatives)
            negs = [x for x in group_negatives]
            random.Random(_hashed_seed).shuffle(negs)
            negs = negs * 2
            negs = negs[_offset: _offset + negative_size]

        return {
            "query": qry,
            "positive_passages": [pos_psg],
            "negative_passages": negs,
        }


@dataclass
class RerankerTrainCollator(DataCollatorWithPadding):
    """
    DataCollator for processing & tokenize train dataset.
    """
    max_length: int = 512

    def __post_init__(self):
        # [SEP] is used for seperate query and passage in DPR
        # We directly use a whitespace ' ' to seperate title & text
        # self.separator = getattr(self.tokenizer, "sep_token", ' ')  # [SEP]
        self.separator = " "        # WhiteSpace
    
    def _get_passage_text(self, item: Dict[str, str]):
        if "title" in item:
            return item["title"] + self.separator + item["text"]
        else:
            return item["text"]

    def __call__(self, features: List[dict]):
        text_pairs: List[List[str]] = list()
        for item in features:
            # Add Query + Positive
            text_pairs.append([item["query"], self._get_passage_text(item["positive_passages"][0])])
            # Add Query + Negatives
            for _neg in item["negative_passages"]:
                text_pairs.append([item["query"], self._get_passage_text(_neg)])    # Add Negative Texts

        # Tokenize
        encoded: BatchEncoding = self.tokenizer(
            text_pairs,
            max_length=self.max_length,
            truncation='longest_first',
            padding=self.padding,
            add_special_tokens=True,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors=self.return_tensors,
        )
        return {"batch": encoded}


class RerankerEncodeDataset(Dataset):
    """ A dataset receives tsv (query_id, passage_id, any) as inputs """
    def __init__(
            self,
            data_args: DataArguments,
            tsv_ranks_path: str,       # Path to .ranks.tsv generated by dual-encoder, format [qid, pid, score]
            query_collection: str,        # Path to query corpus
            passage_collection: str,      # Path to passage corpus
            depth: int=1000,           # Reranking depth, will only re-score `top-depth` per qid using CrossEncoder
    ):
        self.data_args = data_args
        # Load query corpus
        self.query_dataset: datasets.Dataset = self.read_corpus(query_collection)
        self.idx_to_query: Dict[str, int] = self.build_corpus_idx_to_row(self.query_dataset)
        # Load passage corpus
        self.passage_dataset: datasets.Dataset = self.read_corpus(passage_collection)
        self.idx_to_passage: Dict[str, int] = self.build_corpus_idx_to_row(self.passage_dataset)
        # Load query_id, passage_id generated by dual-encoder 
        self.qp_pairs: List[Tuple[str, str]] = self.process_tsv_file(tsv_ranks_path, depth=depth)    # (qid, pid) pairs

    def __len__(self):
        return len(self.qp_pairs) 

    def __getitem__(self, index) -> Dict[str, any]:
        qid, pid = self.qp_pairs[index]
        item = {
            "query_id": qid,
            "query": self.query_dataset[self.idx_to_query[qid]],
            "passage_id": pid,
            "passage": self.passage_dataset[self.idx_to_passage[pid]],
        }
        return item

    @staticmethod
    def build_corpus_idx_to_row(dataset: datasets.Dataset):
        """ Build a dict on memory of corpus id -> row id of hfdataset """
        idx_to_corpus = dict()
        for row_id, corpus_id in enumerate(dataset["_id"]):
            idx_to_corpus[corpus_id] = row_id
        return idx_to_corpus

    @staticmethod
    def read_corpus(corpus_name_or_path: str):
        """ Load HFDataset from local or online """
        corpus_path = None
        # Local file or folders
        if os.path.exists(corpus_name_or_path):
            if os.path.isdir(corpus_name_or_path):
                files = os.listdir(corpus_name_or_path)
                corpus_path = [
                    os.path.join(corpus_name_or_path, f)
                    for f in files
                    if f.endswith('json') or f.endswith('jsonl')
                ]
            else:
                corpus_path = [corpus_name_or_path]
            
            dataset_name = 'json'
            dataset_split = 'train'
            dataset_language = 'default'
        # Online huggingface dataset
        else:
            info = corpus_name_or_path.split('/')
            dataset_split = info[-1] if len(info) == 3 else 'train'
            dataset_name = "/".join(info[:-1]) if len(info) == 3 else '/'.join(info)
            dataset_language = 'default'
            if ':' in dataset_name:
                dataset_name, dataset_language = dataset_name.split(':')
        
        dataset = load_dataset(
            dataset_name,
            dataset_language,
            data_files=corpus_path,
            split=dataset_split
        )

        # Parse tevatron format Jsonl text column names to sentence transformers format
        for _original_column_name, _new_column_name in [("query_id", "_id"), ("docid", "_id"), ("query", "text")]:
            if _original_column_name in dataset.column_names:
                dataset = dataset.rename_column(_original_column_name, _new_column_name)
        
        return dataset
    
    def process_tsv_file(self, tsv_ranks_path: str, depth: int=1000):
        q_p_dict = {}
        ret = []    # (qid, pid) pairs
        print(f"Reading idx from {tsv_ranks_path}")
        for _, line in enumerate(tqdm(open(tsv_ranks_path, 'r'), total=wc_count(tsv_ranks_path))):
            line: list = line.split('\t')
            if len(line) == 4:
                line.pop(1)
            qid: str = line[0].strip()
            pid: str = line[1].strip()
            score: str = float(line[2])     # This score is generated by dual-encoder
            if qid not in q_p_dict:
                q_p_dict[qid] = [(pid, score)]
            else:
                q_p_dict[qid].append((pid, score))
        for k, v in q_p_dict.items():
            q_p_dict[k] = sorted(v, key=lambda x: x[1], reverse=True)
            q_p_dict[k] = q_p_dict[k][:depth]
            ret.extend([(k, pid) for pid, score in q_p_dict[k]]) 
        return ret
    
    def shard_(self, num_shards: int, index: int):
        """ In-place shard operation """
        div = len(self) // num_shards
        mod = len(self) % num_shards
        start = div * index + min(index, mod)
        end = start + div + (1 if index < mod else 0)
        self.qp_pairs = self.qp_pairs[start: end]


@dataclass
class RerankerEncodeCollator(DataCollatorWithPadding):
    """
    DataCollator for processing & tokenize encode dataset.
    """
    max_length: int = 512

    def __post_init__(self):
        # [SEP] is used for seperate query and passage in CrossEncoder
        # We directly use a whitespace ' ' to seperate title & text
        # self.separator = getattr(self.tokenizer, "sep_token", ' ')  # [SEP]
        self.separator = " "        # WhiteSpace
    
    def _get_passage_text(self, item: Dict[str, str]):
        if "title" in item:
            return item["title"] + self.separator + item["text"]
        else:
            return item["text"]

    def __call__(self, features: List[dict]):
        query_id: List[str] = list()
        passage_id: List[str] = list()
        texts: List[List[str]] = list()

        for item in features:
            query_id.append(item["query_id"])
            passage_id.append(item["passage_id"])
            texts.append([self._get_passage_text(item["query"]), self._get_passage_text(item["passage"])])
        
        encoded: BatchEncoding = self.tokenizer(
            texts,
            max_length=self.max_length,
            truncation='longest_first',
            padding=self.padding,
            add_special_tokens=True,
            return_attention_mask=True,
            return_token_type_ids=True,
            return_tensors=self.return_tensors,
        )
        return query_id, passage_id, encoded

